{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4469b602",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 52\u001b[0m\n\u001b[1;32m     47\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOADING DATA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m LOAD_PARAM \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     49\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpin_memory\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m}\n\u001b[0;32m---> 52\u001b[0m train, valid, test \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_training_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPARAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m train_set \u001b[38;5;241m=\u001b[39m PDB_dataset(\u001b[38;5;28mlist\u001b[39m(train\u001b[38;5;241m.\u001b[39mkeys()), loader_pdb, train, PARAMS)\n\u001b[1;32m     54\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_set, worker_init_fn\u001b[38;5;241m=\u001b[39mworker_init_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mLOAD_PARAM)\n",
      "File \u001b[0;32m~/project/side_chain_modeling/../ProteinMPNN/training/utils.py:327\u001b[0m, in \u001b[0;36mbuild_training_clusters\u001b[0;34m(params, debug)\u001b[0m\n\u001b[1;32m    325\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mnext\u001b[39m(reader)\n\u001b[0;32m--> 327\u001b[0m     rows \u001b[38;5;241m=\u001b[39m [[r[\u001b[38;5;241m0\u001b[39m],r[\u001b[38;5;241m3\u001b[39m],\u001b[38;5;28mint\u001b[39m(r[\u001b[38;5;241m4\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m reader\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(r[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRESCUT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    329\u001b[0m             parser\u001b[38;5;241m.\u001b[39mparse(r[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39mparser\u001b[38;5;241m.\u001b[39mparse(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATCUT\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# compile training and validation sets\u001b[39;00m\n\u001b[1;32m    332\u001b[0m train \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/project/side_chain_modeling/../ProteinMPNN/training/utils.py:329\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    325\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mnext\u001b[39m(reader)\n\u001b[1;32m    327\u001b[0m     rows \u001b[38;5;241m=\u001b[39m [[r[\u001b[38;5;241m0\u001b[39m],r[\u001b[38;5;241m3\u001b[39m],\u001b[38;5;28mint\u001b[39m(r[\u001b[38;5;241m4\u001b[39m])] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m reader\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(r[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRESCUT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m             parser\u001b[38;5;241m.\u001b[39mparse(r[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDATCUT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# compile training and validation sets\u001b[39;00m\n\u001b[1;32m    332\u001b[0m train \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:1368\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser(parserinfo)\u001b[38;5;241m.\u001b[39mparse(timestr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULTPARSER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:640\u001b[0m, in \u001b[0;36mparser.parse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    637\u001b[0m     default \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mreplace(hour\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, minute\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    638\u001b[0m                                               second\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, microsecond\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 640\u001b[0m res, skipped_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown string format: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, timestr)\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:719\u001b[0m, in \u001b[0;36mparser._parse\u001b[0;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[1;32m    716\u001b[0m     yearfirst \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39myearfirst\n\u001b[1;32m    718\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result()\n\u001b[0;32m--> 719\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43m_timelex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestr\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# Splits the timestr into tokens\u001b[39;00m\n\u001b[1;32m    721\u001b[0m skipped_idxs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    723\u001b[0m \u001b[38;5;66;03m# year/month/day list\u001b[39;00m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:201\u001b[0m, in \u001b[0;36m_timelex.split\u001b[0;34m(cls, s)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mcls\u001b[39m, s):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:190\u001b[0m, in \u001b[0;36m_timelex.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 190\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/ohpc/pub/apps/anaconda3/lib/python3.9/site-packages/dateutil/parser/_parser.py:181\u001b[0m, in \u001b[0;36m_timelex.get_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tok:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenstack\u001b[38;5;241m.\u001b[39mappend(tok)\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    182\u001b[0m     token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ProteinMPNN.training.utils import build_training_clusters, StructureDataset, StructureLoader, \\\n",
    "                        PDB_dataset, loader_pdb, worker_init_fn\n",
    "from ProteinMPNN.training.model_utils import get_std_opt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import scipy\n",
    "import scipy.spatial\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import queue\n",
    "import time\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \",device)\n",
    "\n",
    "from util import get_coords6d, _dihedrals, _normalize, get_pdbs, generate_Cbeta,\\\n",
    "    get_dihedrals, get_angles, featurize\n",
    "from util_module import XYZConverter\n",
    "from model import ProteinMPNN\n",
    "from loss import torsionAngleLoss\n",
    "\n",
    "LOCAL_PATH = \"/home/minsu/CLIPP/training_data/pdb_2021aug02_sample\"\n",
    "DATA_PATH = \"/public_data/ml/RF2_train/PDB-2021AUG02\"\n",
    "MY_LOCAL = \"/home/dohyun/project/\"\n",
    "PARAMS = {\n",
    "    \"LIST\"    : f\"{MY_LOCAL}/train_s\",\n",
    "    \"VAL\"     : f\"{DATA_PATH}/PDB_val\",\n",
    "    \"TEST\"    : f\"{LOCAL_PATH}/test_clusters.txt\",\n",
    "    \"STRUCT_CLUST\" : f\"{LOCAL_PATH}/seq_hash_to_clust_hash.yaml\",  \n",
    "    \"DIR\"     : f\"{DATA_PATH}/torch\",\n",
    "    \"DATCUT\"  : \"2030-Jan-01\",\n",
    "    \"RESCUT\"  : 3.5,\n",
    "    \"HOMO\"    : 0.70, # min sequence identity for homologous chains\n",
    "    \"CHAIN_ONLY\": False,\n",
    "    \"HARD\"    : f\"{LOCAL_PATH}/hard_negative.yaml\",\n",
    "}\n",
    "logging.info(\"LOADING DATA\")\n",
    "LOAD_PARAM = {'batch_size': 1,\n",
    "              'shuffle': True,\n",
    "              'pin_memory':False,\n",
    "              'num_workers': 4}\n",
    "train, valid, test = build_training_clusters(PARAMS, False)\n",
    "train_set = PDB_dataset(list(train.keys()), loader_pdb, train, PARAMS)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
    "valid_set = PDB_dataset(list(valid.keys()), loader_pdb, valid, PARAMS)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, worker_init_fn=worker_init_fn, **LOAD_PARAM)\n",
    "\n",
    "\n",
    "num_examples_per_epoch=50\n",
    "\n",
    "hidden_dim=128\n",
    "num_encoder_layers=3\n",
    "num_neighbors=32\n",
    "dropout=0.1\n",
    "backbone_noise=0.2\n",
    "max_protein_length = 10000\n",
    "batch_size = 10000\n",
    "reload_data_every_n_epochs = 2\n",
    "mixed_precision = True\n",
    "epoch = 0\n",
    "gradient_norm = 1.0\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "logfile = 'log.txt'\n",
    "model = ProteinMPNN(node_features=hidden_dim, \n",
    "                        edge_features=hidden_dim, \n",
    "                        hidden_dim=hidden_dim, \n",
    "                        num_encoder_layers=num_encoder_layers, \n",
    "                        num_decoder_layers=num_encoder_layers, \n",
    "                        k_neighbors=num_neighbors, \n",
    "                        dropout=dropout, \n",
    "                        augment_eps=backbone_noise)\n",
    "model.to(device)\n",
    "total_step = 0\n",
    "optimizer = get_std_opt(model.parameters(), hidden_dim, total_step)\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor    \n",
    "\n",
    "with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "    q = queue.Queue(maxsize=3)\n",
    "    p = queue.Queue(maxsize=3)\n",
    "\n",
    "    for i in range(3):\n",
    "        q.put_nowait(executor.submit(get_pdbs, train_loader, 1, max_protein_length, num_examples_per_epoch))\n",
    "        p.put_nowait(executor.submit(get_pdbs, valid_loader, 1, max_protein_length, num_examples_per_epoch))\n",
    "    pdb_dict_train = q.get().result()\n",
    "    pdb_dict_valid = p.get().result()\n",
    "    dataset_train = StructureDataset(pdb_dict_train, truncate=None, max_length=max_protein_length) \n",
    "    dataset_valid = StructureDataset(pdb_dict_valid, truncate=None, max_length=max_protein_length)\n",
    "\n",
    "    loader_train = StructureLoader(dataset_train, batch_size=batch_size)\n",
    "    loader_valid = StructureLoader(dataset_valid, batch_size=batch_size)\n",
    "   \n",
    "    for e in range(10):\n",
    "        t0 = time.time()\n",
    "        e = epoch + e\n",
    "        avg_loss = 0.0\n",
    "        model.train()\n",
    "        start_batch = time.time()\n",
    "        for _, batch in enumerate(loader_train):\n",
    "            dist_ca, omega, theta, phi, dihedral, mask_angle, mask, S, chain_M, residue_idx,\\\n",
    "                                chain_encoding_all = featurize(batch, device)\n",
    "            optimizer.zero_grad()\n",
    "            mask_for_loss = mask*chain_M\n",
    "            alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "            alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "            tors = []\n",
    "            for s in range(len(batch)):\n",
    "                all_chains = batch[s]['visible_list']+batch[s]['masked_list']\n",
    "                coord = torch.cat([batch[s][f'coords_chain_{letter}']['xyz_coords'] for letter in all_chains])\n",
    "                all_sequence = batch[s]['seq']\n",
    "                indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "                seq_aa = indices\n",
    "                true_tors, true_tors_alt, tors_mask, tors_planar = XYZConverter().get_torsions(\n",
    "                            torch.unsqueeze(coord,0), \n",
    "                            torch.unsqueeze(torch.from_numpy(seq_aa),0).to(dtype=torch.long)\n",
    "                )\n",
    "                tors.append([true_tors, true_tors_alt, tors_mask, tors_planar])\n",
    "            \n",
    "            if mixed_precision:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    result = model(dist_ca, omega, theta, phi, dihedral, mask_angle, mask, \\\n",
    "                                    S, chain_M, residue_idx, chain_encoding_all)\n",
    "                    l_tors_sum = 0\n",
    "                    for s in range(len(batch)):\n",
    "                        nres = len(batch[s]['seq'])\n",
    "                        true_tors, true_tors_alt, tors_mask, tors_planar = tors[s]\n",
    "                        l_tors = torsionAngleLoss(result[s][:nres].unsqueeze(0), true_tors, true_tors_alt, \\\n",
    "                                                    tors_mask, tors_planar, eps = 1e-10)\n",
    "                        l_tors_sum += l_tors\n",
    "#                     _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)\n",
    "\n",
    "                scaler.scale(l_tors).backward()\n",
    "\n",
    "                if gradient_norm > 0.0:\n",
    "                    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_norm)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                avg_loss += l_tors.detach()\n",
    "            else:\n",
    "                result = model(dist_ca, omega, theta, phi, dihedral, mask_angle, mask, \\\n",
    "                                S, chain_M, residue_idx, chain_encoding_all)\n",
    "                l_tors_sum = 0\n",
    "                for s in range(len(batch)):\n",
    "                    nres = len(batch[s]['seq'])\n",
    "                    true_tors, true_tors_alt, tors_mask, tors_planar = tors[s]\n",
    "                    l_tors = torsionAngleLoss(result[s][:nres].unsqueeze(0), true_tors, true_tors_alt, \\\n",
    "                                                tors_mask, tors_planar, eps = 1e-10)\n",
    "                    l_tors_sum += l_tors\n",
    "                l_tors.backward()\n",
    "\n",
    "                if gradient_norm > 0.0:\n",
    "                    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_norm)\n",
    "                optimizer.step()\n",
    "                avg_loss += l_tors.detach()\n",
    "        elapsed_featurize = time.time() - start_batch\n",
    "            \n",
    "        avg_loss = avg_loss / len(loader_train)\n",
    "        print (\"Train epoch{}, time {:.2f}, loss {} \".format(e, elapsed_featurize, avg_loss.item()))\n",
    "\n",
    "        model.eval()\n",
    "        avg_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(loader_valid):\n",
    "                dist_ca, omega, theta, phi, dihedral, mask_angle, mask, S, chain_M, residue_idx,\\\n",
    "                                chain_encoding_all = featurize(batch, device)\n",
    "                result = model(dist_ca, omega, theta, phi, dihedral, mask_angle, mask, \\\n",
    "                                    S, chain_M, residue_idx, chain_encoding_all)\n",
    "                tors = []\n",
    "                for s in range(len(batch)):\n",
    "                    all_chains = batch[s]['visible_list']+batch[s]['masked_list']\n",
    "                    coord = torch.cat([batch[s][f'coords_chain_{letter}']['xyz_coords'] for letter in all_chains])\n",
    "                    all_sequence = batch[s]['seq']\n",
    "                    indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "                    seq_aa = indices\n",
    "                    true_tors, true_tors_alt, tors_mask, tors_planar = XYZConverter().get_torsions(\n",
    "                                torch.unsqueeze(coord,0), \n",
    "                                torch.unsqueeze(torch.from_numpy(seq_aa),0).to(dtype=torch.long)\n",
    "                    )\n",
    "                    tors.append([true_tors, true_tors_alt, tors_mask, tors_planar])\n",
    "                \n",
    "                l_tors_sum = 0\n",
    "                for s in range(len(batch)):\n",
    "                    nres = len(batch[s]['seq'])\n",
    "                    true_tors, true_tors_alt, tors_mask, tors_planar = tors[s]\n",
    "                    l_tors = torsionAngleLoss(result[s][:nres].unsqueeze(0), true_tors, true_tors_alt, \\\n",
    "                                                tors_mask, tors_planar, eps = 1e-10)\n",
    "                    l_tors_sum += l_tors\n",
    "                avg_loss += loss.detach()\n",
    "        avg_loss = avg_loss / len(loader_valid)\n",
    "        print (\"valid epoch {}, loss {} \".format(e, avg_loss.item()))\n",
    "        \n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                }, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7918ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch0, time 13.85, loss 1.2098737955093384 \n",
      "valid epoch0, loss 8.349197387695312 \n",
      "Train epoch1, time 13.63, loss 0.8898345828056335 \n",
      "valid epoch1, loss 6.537850856781006 \n",
      "Train epoch2, time 13.19, loss 0.6854068040847778 \n",
      "valid epoch2, loss 5.646066188812256 \n",
      "Train epoch3, time 13.51, loss 0.6019771099090576 \n",
      "valid epoch3, loss 5.407088756561279 \n",
      "Train epoch4, time 12.87, loss 0.5594022870063782 \n",
      "valid epoch4, loss 5.039198875427246 \n",
      "Train epoch5, time 13.56, loss 0.5295993685722351 \n",
      "valid epoch5, loss 4.806640148162842 \n",
      "Train epoch6, time 13.73, loss 0.5121631026268005 \n",
      "valid epoch6, loss 4.693861961364746 \n",
      "Train epoch7, time 12.96, loss 0.5042111277580261 \n",
      "valid epoch7, loss 4.5997724533081055 \n",
      "Train epoch8, time 13.47, loss 0.5048273205757141 \n",
      "valid epoch8, loss 4.701231956481934 \n",
      "Train epoch9, time 12.77, loss 0.5055730938911438 \n",
      "valid epoch9, loss 4.61162805557251 \n"
     ]
    }
   ],
   "source": [
    "mixed_precision = False\n",
    "for e in range(10):\n",
    "        t0 = time.time()\n",
    "        e = epoch + e\n",
    "        avg_loss = 0.0\n",
    "        model.train()\n",
    "        start_batch = time.time()\n",
    "        for _, batch in enumerate(loader_train):\n",
    "            dist_ca, omega, theta, phi, dihedral, mask_angle, mask, S, chain_M, residue_idx,\\\n",
    "                                chain_encoding_all = featurize(batch, device)\n",
    "            optimizer.zero_grad()\n",
    "            mask_for_loss = mask*chain_M\n",
    "            alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "            alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "            tors = []\n",
    "            for s in range(len(batch)):\n",
    "                all_chains = batch[s]['visible_list']+batch[s]['masked_list']\n",
    "                coord = torch.cat([batch[s][f'coords_chain_{letter}']['xyz_coords'] for letter in all_chains])\n",
    "                all_sequence = batch[s]['seq']\n",
    "                indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "                seq_aa = indices\n",
    "                true_tors, true_tors_alt, tors_mask, tors_planar = XYZConverter().get_torsions(\n",
    "                            torch.unsqueeze(coord,0), \n",
    "                            torch.unsqueeze(torch.from_numpy(seq_aa),0).to(dtype=torch.long)\n",
    "                )\n",
    "                tors.append([true_tors, true_tors_alt, tors_mask, tors_planar])\n",
    "            \n",
    "#             if mixed_precision:\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "#                     result = model(dist_ca, omega, theta, phi, dihedral, mask_angle, mask, \\\n",
    "#                                     S, chain_M, residue_idx, chain_encoding_all)\n",
    "#                     l_tors_sum = 0\n",
    "#                     for s in range(len(batch)):\n",
    "#                         nres = len(batch[s]['seq'])\n",
    "#                         true_tors, true_tors_alt, tors_mask, tors_planar = tors[s]\n",
    "#                         l_tors = torsionAngleLoss(result[s][:nres].unsqueeze(0), true_tors, true_tors_alt, \\\n",
    "#                                                     tors_mask, tors_planar, eps = 1e-10)\n",
    "#                         l_tors_sum += l_tors\n",
    "# #                     _, loss_av_smoothed = loss_smoothed(S, log_probs, mask_for_loss)\n",
    "\n",
    "#                 scaler.scale(l_tors).backward()\n",
    "\n",
    "#                 if gradient_norm > 0.0:\n",
    "#                     total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_norm)\n",
    "\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#                 avg_loss += l_tors.detach()\n",
    "#             else:\n",
    "            result = model(dist_ca, omega, theta, phi, dihedral, mask_angle, mask, \\\n",
    "                            S, chain_M, residue_idx, chain_encoding_all)\n",
    "            l_tors_sum = 0\n",
    "            for s in range(len(batch)):\n",
    "                nres = len(batch[s]['seq'])\n",
    "                true_tors, true_tors_alt, tors_mask, tors_planar = tors[s]\n",
    "                l_tors = torsionAngleLoss(result[s][:nres].unsqueeze(0), true_tors, true_tors_alt, \\\n",
    "                                            tors_mask, tors_planar, eps = 1e-10)\n",
    "                l_tors_sum += l_tors\n",
    "            l_tors.backward()\n",
    "\n",
    "            if gradient_norm > 0.0:\n",
    "                total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_norm)\n",
    "            optimizer.step()\n",
    "            avg_loss += l_tors.detach()\n",
    "        elapsed_featurize = time.time() - start_batch\n",
    "            \n",
    "        avg_loss = avg_loss / len(loader_train)\n",
    "        print (\"Train epoch{}, time {:.2f}, loss {} \".format(e, elapsed_featurize, avg_loss.item()))\n",
    "\n",
    "        model.eval()\n",
    "        avg_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for _, batch in enumerate(loader_valid):\n",
    "                dist_ca, omega, theta, phi, dihedral, mask_angle, mask, S, chain_M, residue_idx,\\\n",
    "                                chain_encoding_all = featurize(batch, device)\n",
    "                result = model(dist_ca, omega, theta, phi, dihedral, mask_angle, mask, \\\n",
    "                                    S, chain_M, residue_idx, chain_encoding_all)\n",
    "                tors = []\n",
    "                for s in range(len(batch)):\n",
    "                    all_chains = batch[s]['visible_list']+batch[s]['masked_list']\n",
    "                    coord = torch.cat([batch[s][f'coords_chain_{letter}']['xyz_coords'] for letter in all_chains])\n",
    "                    all_sequence = batch[s]['seq']\n",
    "                    indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
    "                    seq_aa = indices\n",
    "                    true_tors, true_tors_alt, tors_mask, tors_planar = XYZConverter().get_torsions(\n",
    "                                torch.unsqueeze(coord,0), \n",
    "                                torch.unsqueeze(torch.from_numpy(seq_aa),0).to(dtype=torch.long)\n",
    "                    )\n",
    "                    tors.append([true_tors, true_tors_alt, tors_mask, tors_planar])\n",
    "                \n",
    "                l_tors_sum = 0\n",
    "                for s in range(len(batch)):\n",
    "                    nres = len(batch[s]['seq'])\n",
    "                    true_tors, true_tors_alt, tors_mask, tors_planar = tors[s]\n",
    "                    l_tors = torsionAngleLoss(result[s][:nres].unsqueeze(0), true_tors, true_tors_alt, \\\n",
    "                                                tors_mask, tors_planar, eps = 1e-10)\n",
    "                    l_tors_sum += l_tors\n",
    "                avg_loss += l_tors_sum.detach()\n",
    "        avg_loss = avg_loss / len(loader_valid)\n",
    "        print (\"valid epoch{}, loss {} \".format( e, avg_loss.item()))\n",
    "        \n",
    "        torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                }, 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
